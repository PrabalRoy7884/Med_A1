{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Prediction - Data Preparation\n",
    "\n",
    "This notebook handles data loading, cleaning, and initial preprocessing for the disease prediction hackathon project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.preprocess import DataPreprocessor\n",
    "from utils.helpers import load_data, plot_class_distribution\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "train_data = load_data('../data/raw/Training.csv')\n",
    "test_data = load_data('../data/raw/Testing.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"TRAINING DATA INFO:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {train_data.shape}\")\n",
    "print(f\"Columns: {train_data.columns.tolist()[:10]}... (showing first 10)\")\n",
    "print(f\"Data types: {train_data.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Memory usage: {train_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nTESTING DATA INFO:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {test_data.shape}\")\n",
    "print(f\"Data types: {test_data.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Memory usage: {test_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"MISSING VALUES ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_missing = train_data.isnull().sum()\n",
    "test_missing = test_data.isnull().sum()\n",
    "\n",
    "print(f\"Training data missing values: {train_missing.sum()}\")\n",
    "print(f\"Testing data missing values: {test_missing.sum()}\")\n",
    "\n",
    "if train_missing.sum() > 0:\n",
    "    print(\"\\nColumns with missing values in training data:\")\n",
    "    print(train_missing[train_missing > 0])\n",
    "    \n",
    "if test_missing.sum() > 0:\n",
    "    print(\"\\nColumns with missing values in testing data:\")\n",
    "    print(test_missing[test_missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the target variable (prognosis)\n",
    "print(\"TARGET VARIABLE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Number of unique diseases in training data: {train_data['prognosis'].nunique()}\")\n",
    "print(f\"Number of unique diseases in testing data: {test_data['prognosis'].nunique()}\")\n",
    "\n",
    "print(\"\\nDiseases in training data:\")\n",
    "print(train_data['prognosis'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nDiseases in testing data:\")\n",
    "print(test_data['prognosis'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all diseases in test set are present in train set\n",
    "train_diseases = set(train_data['prognosis'].unique())\n",
    "test_diseases = set(test_data['prognosis'].unique())\n",
    "\n",
    "missing_in_train = test_diseases - train_diseases\n",
    "missing_in_test = train_diseases - test_diseases\n",
    "\n",
    "print(f\"Diseases in test but not in train: {len(missing_in_train)}\")\n",
    "if missing_in_train:\n",
    "    print(f\"Missing diseases: {list(missing_in_train)}\")\n",
    "    \n",
    "print(f\"\\nDiseases in train but not in test: {len(missing_in_test)}\")\n",
    "if missing_in_test:\n",
    "    print(f\"Missing diseases: {list(missing_in_test)[:10]}... (showing first 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Symptom Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get symptom columns (all columns except prognosis)\n",
    "symptom_cols = [col for col in train_data.columns if col != 'prognosis']\n",
    "print(f\"Number of symptom features: {len(symptom_cols)}\")\n",
    "print(f\"First 10 symptoms: {symptom_cols[:10]}\")\n",
    "\n",
    "# Check symptom value distribution\n",
    "print(\"\\nSymptom value distribution in training data:\")\n",
    "for col in symptom_cols[:5]:  # Show first 5 symptoms\n",
    "    print(f\"{col}: {train_data[col].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate symptom frequencies\n",
    "symptom_frequencies = train_data[symptom_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"TOP 20 MOST COMMON SYMPTOMS:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (symptom, freq) in enumerate(symptom_frequencies.head(20).items(), 1):\n",
    "    percentage = (freq / len(train_data)) * 100\n",
    "    print(f\"{i:2d}. {symptom:30s}: {freq:4d} ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize symptom frequencies\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Top 30 symptoms\n",
    "plt.subplot(2, 1, 1)\n",
    "top_30_symptoms = symptom_frequencies.head(30)\n",
    "top_30_symptoms.plot(kind='barh')\n",
    "plt.title('Top 30 Most Common Symptoms')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Symptoms')\n",
    "\n",
    "# Symptom frequency distribution\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(symptom_frequencies.values, bins=50, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Symptom Frequencies')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of Symptoms')\n",
    "plt.axvline(symptom_frequencies.mean(), color='red', linestyle='--', label=f'Mean: {symptom_frequencies.mean():.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"DUPLICATE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_duplicates = train_data.duplicated().sum()\n",
    "test_duplicates = test_data.duplicated().sum()\n",
    "\n",
    "print(f\"Duplicate rows in training data: {train_duplicates}\")\n",
    "print(f\"Duplicate rows in testing data: {test_duplicates}\")\n",
    "\n",
    "if train_duplicates > 0:\n",
    "    print(f\"Percentage of duplicates in training data: {(train_duplicates/len(train_data))*100:.2f}%\")\n",
    "    \n",
    "if test_duplicates > 0:\n",
    "    print(f\"Percentage of duplicates in testing data: {(test_duplicates/len(test_data))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check symptom patterns per disease\n",
    "print(\"SYMPTOM PATTERNS ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Average number of symptoms per patient\n",
    "symptoms_per_patient = train_data[symptom_cols].sum(axis=1)\n",
    "print(f\"Average symptoms per patient: {symptoms_per_patient.mean():.2f}\")\n",
    "print(f\"Min symptoms per patient: {symptoms_per_patient.min()}\")\n",
    "print(f\"Max symptoms per patient: {symptoms_per_patient.max()}\")\n",
    "\n",
    "# Distribution of symptom counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(symptoms_per_patient, bins=20, alpha=0.7, color='lightgreen')\n",
    "plt.title('Distribution of Symptoms per Patient')\n",
    "plt.xlabel('Number of Symptoms')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.axvline(symptoms_per_patient.mean(), color='red', linestyle='--', label=f'Mean: {symptoms_per_patient.mean():.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(symptoms_per_patient)\n",
    "plt.title('Boxplot of Symptoms per Patient')\n",
    "plt.ylabel('Number of Symptoms')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Load data through preprocessor\n",
    "train_df, test_df = preprocessor.load_data('../data/raw/Training.csv', '../data/raw/Testing.csv')\n",
    "\n",
    "print(\"\\nData loaded through preprocessor successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X_train, y_train = preprocessor.prepare_features_and_target()\n",
    "X_test, y_test = preprocessor.prepare_features_and_target(test_df)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nUnique encoded labels: {np.unique(y_train)[:10]}... (showing first 10)\")\n",
    "print(f\"Disease names: {preprocessor.get_disease_names()[:5]}... (showing first 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "from collections import Counter\n",
    "\n",
    "class_distribution = Counter(y_train)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Number of classes: {len(class_distribution)}\")\n",
    "print(f\"Min samples per class: {min(class_distribution.values())}\")\n",
    "print(f\"Max samples per class: {max(class_distribution.values())}\")\n",
    "print(f\"Average samples per class: {np.mean(list(class_distribution.values())):.1f}\")\n",
    "\n",
    "# Check if classes are balanced\n",
    "class_counts = list(class_distribution.values())\n",
    "is_balanced = max(class_counts) - min(class_counts) <= 1\n",
    "print(f\"\\nClasses are balanced: {is_balanced}\")\n",
    "\n",
    "if not is_balanced:\n",
    "    print(\"Recommendation: Consider using class balancing techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "disease_names = preprocessor.get_disease_names()\n",
    "disease_counts = [class_distribution[i] for i in range(len(disease_names))]\n",
    "\n",
    "plt.bar(range(len(disease_names)), disease_counts)\n",
    "plt.title('Disease Class Distribution in Training Data')\n",
    "plt.xlabel('Disease Index')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(range(0, len(disease_names), 5))  # Show every 5th tick\n",
    "\n",
    "# Add horizontal line for average\n",
    "avg_count = np.mean(disease_counts)\n",
    "plt.axhline(y=avg_count, color='r', linestyle='--', label=f'Average: {avg_count:.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Validation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation\n",
    "print(\"DATA VALIDATION SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "validations = []\n",
    "\n",
    "# Check 1: Correct number of features\n",
    "expected_features = 132\n",
    "actual_features = X_train.shape[1]\n",
    "validations.append((f\"Feature count ({expected_features}\", actual_features == expected_features))\n",
    "\n",
    "# Check 2: Correct number of classes\n",
    "expected_classes = 42\n",
    "actual_classes = len(np.unique(y_train))\n",
    "validations.append((f\"Class count ({expected_classes}\", actual_classes == expected_classes))\n",
    "\n",
    "# Check 3: No missing values\n",
    "no_missing = not np.isnan(X_train).any()\n",
    "validations.append((\"No missing values\", no_missing))\n",
    "\n",
    "# Check 4: Binary features (0 or 1)\n",
    "binary_features = np.all(np.isin(X_train, [0, 1]))\n",
    "validations.append((\"Binary features (0/1)\", binary_features))\n",
    "\n",
    "# Check 5: Consistent test set\n",
    "consistent_test = X_test.shape[1] == X_train.shape[1]\n",
    "validations.append((\"Consistent test set features\", consistent_test))\n",
    "\n",
    "# Print validation results\n",
    "for validation, passed in validations:\n",
    "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "    print(f\"{status} - {validation}\")\n",
    "\n",
    "all_passed = all(result[1] for result in validations)\n",
    "print(f\"\\nOverall validation: {'✅ ALL CHECKS PASSED' if all_passed else '❌ SOME CHECKS FAILED'}\")\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n🎉 Data preparation completed successfully!\")\n",
    "    print(\"Ready for exploratory data analysis and model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessor object for later use\n",
    "import joblib\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, '../models/preprocessor.pkl')\n",
    "print(\"Preprocessor saved to '../models/preprocessor.pkl'\")\n",
    "\n",
    "# Save basic processed data\n",
    "processed_train = pd.DataFrame(X_train, columns=preprocessor.get_symptom_names())\n",
    "processed_train['prognosis'] = y_train\n",
    "\n",
    "processed_test = pd.DataFrame(X_test, columns=preprocessor.get_symptom_names())\n",
    "processed_test['prognosis'] = y_test\n",
    "\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "processed_train.to_csv('../data/processed/train_encoded.csv', index=False)\n",
    "processed_test.to_csv('../data/processed/test_encoded.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved to '../data/processed/'\")\n",
    "print(\"\\n📊 Data preparation notebook completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}