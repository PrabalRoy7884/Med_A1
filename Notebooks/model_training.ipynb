{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Prediction - Model Training and Evaluation\n",
    "\n",
    "This notebook implements comprehensive model training with hyperparameter optimization, cross-validation, and model evaluation for the disease prediction hackathon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Model training environment setup completed!\n",
      "Timestamp: 2025-09-09 11:28:34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.train import DiseasePredictor\n",
    "from utils.helpers import *\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           classification_report, confusion_matrix, roc_auc_score)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import optuna\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"🚀 Model training environment setup completed!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and recreating preprocessor...\n",
      "Raw training data shape: (4920, 133)\n",
      "Raw test data shape: (42, 133)\n",
      "Number of symptom features: 132\n",
      "Number of unique diseases in training: 42\n",
      "Number of unique diseases in test: 42\n",
      "Total unique diseases: 42\n",
      "Label encoder fitted on 42 disease classes\n",
      "Disease classes: ['(vertigo) Paroymsal  Positional Vertigo', 'AIDS', 'Acne', 'Alcoholic hepatitis', 'Allergy', 'Arthritis', 'Bronchial Asthma', 'Cervical spondylosis', 'Chicken pox', 'Chronic cholestasis']...\n"
     ]
    }
   ],
   "source": [
    "# Load the raw data directly and recreate preprocessor\n",
    "print(\"Loading data and recreating preprocessor...\")\n",
    "\n",
    "# Load raw data\n",
    "train_data_raw = pd.read_csv('../data/raw/Training.csv')\n",
    "test_data_raw = pd.read_csv('../data/raw/Testing.csv')\n",
    "\n",
    "print(f\"Raw training data shape: {train_data_raw.shape}\")\n",
    "print(f\"Raw test data shape: {test_data_raw.shape}\")\n",
    "\n",
    "# Get symptom columns (all except 'prognosis')\n",
    "symptom_columns = [col for col in train_data_raw.columns if col != 'prognosis']\n",
    "print(f\"Number of symptom features: {len(symptom_columns)}\")\n",
    "\n",
    "# Check unique diseases\n",
    "train_diseases = set(train_data_raw['prognosis'].unique())\n",
    "test_diseases = set(test_data_raw['prognosis'].unique())\n",
    "all_diseases = sorted(list(train_diseases.union(test_diseases)))\n",
    "\n",
    "print(f\"Number of unique diseases in training: {len(train_diseases)}\")\n",
    "print(f\"Number of unique diseases in test: {len(test_diseases)}\")\n",
    "print(f\"Total unique diseases: {len(all_diseases)}\")\n",
    "\n",
    "# Create new label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_diseases)  # Fit on all diseases\n",
    "\n",
    "print(f\"Label encoder fitted on {len(label_encoder.classes_)} disease classes\")\n",
    "print(f\"Disease classes: {list(label_encoder.classes_)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\venv\\lib\\site-packages\\sklearn\\utils\\_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\venv\\lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\venv\\lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\venv\\lib\\site-packages\\sklearn\\utils\\_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 15",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare features and targets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_features_and_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mprepare_features_and_target(test_data)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature matrix shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\Notebooks\\../src\\data\\preprocess.py:58\u001b[0m, in \u001b[0;36mDataPreprocessor.prepare_features_and_target\u001b[1;34m(self, data, encode_target)\u001b[0m\n\u001b[0;32m     55\u001b[0m X \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymptom_columns]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encode_target:\n\u001b[1;32m---> 58\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_column]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\Notebooks\\../src\\data\\preprocess.py:47\u001b[0m, in \u001b[0;36mDataPreprocessor.encode_labels\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_column])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Transform the labels\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m encoded_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_labels\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\port\\diesase\\venv\\lib\\site-packages\\sklearn\\utils\\_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 15"
     ]
    }
   ],
   "source": [
    "# Prepare features and targets\n",
    "X, y = preprocessor.prepare_features_and_target(train_data)\n",
    "X_test, y_test = preprocessor.prepare_features_and_target(test_data)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Test matrix shape: {X_test.shape}\")\n",
    "\n",
    "# Feature selection based on EDA insights\n",
    "print(f\"\\n🔍 Applying feature selection...\")\n",
    "\n",
    "# Use chi-square test for feature selection\n",
    "selector = SelectKBest(score_func=chi2, k=100)  # Select top 100 features\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "selected_features = selector.get_support()\n",
    "selected_feature_names = [name for name, selected in zip(preprocessor.get_symptom_names(), selected_features) if selected]\n",
    "\n",
    "print(f\"Selected {len(selected_feature_names)} features out of {len(preprocessor.get_symptom_names())}\")\n",
    "print(f\"Selected features shape: {X_selected.shape}\")\n",
    "\n",
    "# Use selected features\n",
    "X_train_final = X_selected\n",
    "X_test_final = X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train-validation split...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create train-validation split\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating train-validation split...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mX_train_final\u001b[49m, y, \n\u001b[0;32m      6\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[0;32m      7\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, \n\u001b[0;32m      8\u001b[0m     stratify\u001b[38;5;241m=\u001b[39my\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_val\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_final' is not defined"
     ]
    }
   ],
   "source": [
    "# Create train-validation split\n",
    "print(\"Creating train-validation split...\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_final, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test_final.shape}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "from collections import Counter\n",
    "\n",
    "train_dist = Counter(y_train)\n",
    "val_dist = Counter(y_val)\n",
    "test_dist = Counter(y_test)\n",
    "\n",
    "print(f\"\\nClass distribution (showing first 5 classes):\")\n",
    "for i in range(min(5, len(train_dist))):\n",
    "    disease_name = preprocessor.decode_predictions([i])[0][:20]  # Truncate long names\n",
    "    print(f\"  {disease_name}: Train={train_dist[i]}, Val={val_dist[i]}, Test={test_dist[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 10 models for training\n",
      "🔥 Starting baseline model training...\n",
      "This may take several minutes...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may take several minutes...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train and evaluate all models\u001b[39;00m\n\u001b[0;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mtrain_and_evaluate_models(\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mX_train\u001b[49m, y_train, X_val, y_val, cv_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Baseline training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest baseline model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictor\u001b[38;5;241m.\u001b[39mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the disease predictor\n",
    "predictor = DiseasePredictor()\n",
    "predictor.initialize_models()\n",
    "\n",
    "print(\"🔥 Starting baseline model training...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = predictor.train_and_evaluate_models(\n",
    "    X_train, y_train, X_val, y_val, cv_folds=5\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Baseline training completed!\")\n",
    "print(f\"Best baseline model: {predictor.best_model_name}\")\n",
    "print(f\"Best CV score: {predictor.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BASELINE MODEL RESULTS:\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 BASELINE MODEL RESULTS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      5\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation_Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: [results[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m results],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV_Mean\u001b[39m\u001b[38;5;124m'\u001b[39m: [results[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_mean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m results],\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV_Std\u001b[39m\u001b[38;5;124m'\u001b[39m: [results[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_std\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[0;32m     10\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV_Mean\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m results_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m20s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation_Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV_Mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV_Std\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Display detailed results\n",
    "print(\"📊 BASELINE MODEL RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Validation_Accuracy': [results[model]['accuracy'] for model in results],\n",
    "    'CV_Mean': [results[model]['cv_mean'] for model in results],\n",
    "    'CV_Std': [results[model]['cv_std'] for model in results]\n",
    "}).sort_values('CV_Mean', ascending=False)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['Model']:20s} | Val Acc: {row['Validation_Accuracy']:.4f} | \"\n",
    "          f\"CV: {row['CV_Mean']:.4f} ± {row['CV_Std']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig = px.bar(\n",
    "    results_df, \n",
    "    x='CV_Mean', \n",
    "    y='Model',\n",
    "    orientation='h',\n",
    "    title='Baseline Model Performance (Cross-Validation Accuracy)',\n",
    "    error_x='CV_Std',\n",
    "    height=600\n",
    ")\n",
    "fig.update_layout(xaxis_title='Cross-Validation Accuracy', yaxis={'categoryorder': 'total ascending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for top 3 models\n",
    "top_3_models = results_df.head(3)['Model'].tolist()\n",
    "\n",
    "print(f\"🎯 Starting hyperparameter optimization for top 3 models: {top_3_models}\")\n",
    "\n",
    "optimized_results = {}\n",
    "\n",
    "# Only optimize models that support it\n",
    "optimizable_models = ['Random Forest', 'XGBoost', 'CatBoost']\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    if model_name in optimizable_models:\n",
    "        print(f\"\\n🔧 Optimizing {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            optimized_model, best_params = predictor.optimize_hyperparameters(\n",
    "                X_train, y_train, model_name=model_name, n_trials=30\n",
    "            )\n",
    "            \n",
    "            # Evaluate optimized model\n",
    "            y_val_pred = optimized_model.predict(X_val)\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            optimized_results[f\"{model_name} (Optimized)\"] = {\n",
    "                'model': optimized_model,\n",
    "                'params': best_params,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'cv_score': predictor.best_score if model_name in predictor.best_model_name else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {model_name} optimization completed!\")\n",
    "            print(f\"   Validation accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"   Best parameters: {best_params}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error optimizing {model_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"⏩ Skipping {model_name} (optimization not implemented)\")\n",
    "\n",
    "print(f\"\\n🎉 Hyperparameter optimization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model\n",
    "print(\"🤝 Creating ensemble model...\")\n",
    "\n",
    "try:\n",
    "    ensemble_model = predictor.create_ensemble_model(X_train, y_train)\n",
    "    \n",
    "    # Evaluate ensemble on validation set\n",
    "    y_val_pred_ensemble = ensemble_model.predict(X_val)\n",
    "    ensemble_val_accuracy = accuracy_score(y_val, y_val_pred_ensemble)\n",
    "    \n",
    "    print(f\"✅ Ensemble model created successfully!\")\n",
    "    print(f\"   Validation accuracy: {ensemble_val_accuracy:.4f}\")\n",
    "    print(f\"   CV score: {predictor.best_score:.4f}\")\n",
    "    \n",
    "    # Add to optimized results\n",
    "    optimized_results['Ensemble'] = {\n",
    "        'model': ensemble_model,\n",
    "        'val_accuracy': ensemble_val_accuracy,\n",
    "        'cv_score': predictor.best_score\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating ensemble: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models (baseline + optimized + ensemble)\n",
    "print(\"🏆 FINAL MODEL COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_comparison = []\n",
    "\n",
    "# Add baseline results\n",
    "for model_name, result in results.items():\n",
    "    final_comparison.append({\n",
    "        'Model': model_name,\n",
    "        'Type': 'Baseline',\n",
    "        'CV_Score': result['cv_mean'],\n",
    "        'Val_Accuracy': result['accuracy']\n",
    "    })\n",
    "\n",
    "# Add optimized results\n",
    "for model_name, result in optimized_results.items():\n",
    "    model_type = 'Ensemble' if model_name == 'Ensemble' else 'Optimized'\n",
    "    final_comparison.append({\n",
    "        'Model': model_name,\n",
    "        'Type': model_type,\n",
    "        'CV_Score': result.get('cv_score', 0),\n",
    "        'Val_Accuracy': result['val_accuracy']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(final_comparison).sort_values('Val_Accuracy', ascending=False)\n",
    "\n",
    "print(\"Model Ranking by Validation Accuracy:\")\n",
    "for i, (_, row) in enumerate(comparison_df.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['Model']:25s} ({row['Type']:9s}) | \"\n",
    "          f\"Val: {row['Val_Accuracy']:.4f} | CV: {row['CV_Score']:.4f}\")\n",
    "\n",
    "# Select final model (best validation accuracy)\n",
    "best_model_info = comparison_df.iloc[0]\n",
    "final_model_name = best_model_info['Model']\n",
    "\n",
    "# Get the actual model object\n",
    "if best_model_info['Type'] == 'Baseline':\n",
    "    final_model = results[final_model_name]['model']\n",
    "else:\n",
    "    final_model = optimized_results[final_model_name]['model']\n",
    "\n",
    "print(f\"\\n🎯 FINAL MODEL SELECTED: {final_model_name}\")\n",
    "print(f\"   Type: {best_model_info['Type']}\")\n",
    "print(f\"   Validation Accuracy: {best_model_info['Val_Accuracy']:.4f}\")\n",
    "print(f\"   Cross-Validation Score: {best_model_info['CV_Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation of final model\n",
    "print(f\"\\n🔬 COMPREHENSIVE EVALUATION OF {final_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Predictions on all sets\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "y_test_pred = final_model.predict(X_test_final)\n",
    "\n",
    "# Calculate metrics for all sets\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "val_metrics = calculate_metrics(y_val, y_val_pred)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n📈 PERFORMANCE METRICS:\")\n",
    "print(f\"{'Metric':15s} | {'Train':8s} | {'Val':8s} | {'Test':8s}\")\n",
    "print(\"-\" * 50)\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
    "    print(f\"{metric.title():15s} | {train_metrics[metric]:8.4f} | \"\n",
    "          f\"{val_metrics[metric]:8.4f} | {test_metrics[metric]:8.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfitting_score = train_metrics['accuracy'] - val_metrics['accuracy']\n",
    "if overfitting_score > 0.05:\n",
    "    print(f\"\\n⚠️  Warning: Potential overfitting detected (train-val gap: {overfitting_score:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n✅ Good generalization (train-val gap: {overfitting_score:.4f})\")\n",
    "\n",
    "# Detailed classification report\n",
    "print_classification_report(y_test, y_test_pred, target_names=preprocessor.get_disease_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Train': [train_metrics[m] for m in ['accuracy', 'precision', 'recall', 'f1_score']],\n",
    "    'Validation': [val_metrics[m] for m in ['accuracy', 'precision', 'recall', 'f1_score']],\n",
    "    'Test': [test_metrics[m] for m in ['accuracy', 'precision', 'recall', 'f1_score']]\n",
    "}, index=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "fig = px.bar(\n",
    "    metrics_comparison.T, \n",
    "    title=f'Performance Metrics Comparison - {final_model_name}',\n",
    "    height=500\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title='Dataset',\n",
    "    yaxis_title='Score',\n",
    "    legend_title='Metrics'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from final model\n",
    "print(\"🔍 FEATURE IMPORTANCE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    feature_importance = predictor.get_feature_importance(selected_feature_names)\n",
    "    \n",
    "    if feature_importance is not None:\n",
    "        print(\"Top 20 most important features:\")\n",
    "        for i, (_, row) in enumerate(feature_importance.head(20).iterrows(), 1):\n",
    "            print(f\"{i:2d}. {row['feature']:25s}: {row['importance']:.4f}\")\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        top_20_features = feature_importance.head(20)\n",
    "        \n",
    "        fig = px.bar(\n",
    "            top_20_features,\n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            orientation='h',\n",
    "            title='Top 20 Feature Importance',\n",
    "            height=600\n",
    "        )\n",
    "        fig.update_layout(yaxis={'categoryorder': 'total ascending'})\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"Feature importance not available for this model type.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error getting feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors\n",
    "print(\"🔍 ERROR ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Find misclassified samples in test set\n",
    "misclassified = y_test != y_test_pred\n",
    "correct_classified = y_test == y_test_pred\n",
    "\n",
    "print(f\"Total test samples: {len(y_test)}\")\n",
    "print(f\"Correctly classified: {correct_classified.sum()} ({(correct_classified.sum()/len(y_test)):.1%})\")\n",
    "print(f\"Misclassified: {misclassified.sum()} ({(misclassified.sum()/len(y_test)):.1%})\")\n",
    "\n",
    "if misclassified.sum() > 0:\n",
    "    print(\"\\nMisclassified samples analysis:\")\n",
    "    \n",
    "    misclassified_df = pd.DataFrame({\n",
    "        'True_Disease': preprocessor.decode_predictions(y_test[misclassified]),\n",
    "        'Predicted_Disease': preprocessor.decode_predictions(y_test_pred[misclassified])\n",
    "    })\n",
    "    \n",
    "    print(f\"Unique misclassification patterns: {len(misclassified_df)}\")\n",
    "    \n",
    "    if len(misclassified_df) <= 10:\n",
    "        for i, (_, row) in enumerate(misclassified_df.iterrows(), 1):\n",
    "            print(f\"{i}. True: {row['True_Disease'][:20]} → Predicted: {row['Predicted_Disease'][:20]}\")\n",
    "    else:\n",
    "        print(\"First 5 misclassifications:\")\n",
    "        for i, (_, row) in enumerate(misclassified_df.head(5).iterrows(), 1):\n",
    "            print(f\"{i}. True: {row['True_Disease'][:20]} → Predicted: {row['Predicted_Disease'][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization (for a subset of classes)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disease_names = preprocessor.get_disease_names()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Analysis:\")\n",
    "print(f\"Matrix shape: {cm.shape}\")\n",
    "print(f\"Perfect predictions (diagonal): {np.trace(cm)}\")\n",
    "print(f\"Total predictions: {np.sum(cm)}\")\n",
    "\n",
    "# Since we have many classes, let's show a subset or summary\n",
    "if len(disease_names) <= 15:\n",
    "    # Show full confusion matrix if not too many classes\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[name[:10] for name in disease_names],\n",
    "                yticklabels=[name[:10] for name in disease_names])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Show accuracy per class\n",
    "    class_accuracies = []\n",
    "    for i in range(len(disease_names)):\n",
    "        if cm[i].sum() > 0:  # Avoid division by zero\n",
    "            accuracy = cm[i, i] / cm[i].sum()\n",
    "            class_accuracies.append((disease_names[i], accuracy, cm[i].sum()))\n",
    "    \n",
    "    class_accuracies.sort(key=lambda x: x[1])  # Sort by accuracy\n",
    "    \n",
    "    print(\"\\nPer-class accuracy (worst performing classes):\")\n",
    "    for disease, acc, samples in class_accuracies[:10]:\n",
    "        print(f\"{disease[:25]:25s}: {acc:.3f} ({samples} samples)\")\n",
    "    \n",
    "    print(\"\\nPer-class accuracy (best performing classes):\")\n",
    "    for disease, acc, samples in class_accuracies[-10:]:\n",
    "        print(f\"{disease[:25]:25s}: {acc:.3f} ({samples} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Persistence and Final Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and all necessary artifacts\n",
    "print(\"💾 SAVING MODEL ARTIFACTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Update preprocessor with feature selection information\n",
    "preprocessor.selected_features = selected_features\n",
    "preprocessor.selected_feature_names = selected_feature_names\n",
    "preprocessor.feature_selector = selector\n",
    "\n",
    "# Save the best model with all necessary information\n",
    "predictor.best_model = final_model\n",
    "predictor.best_model_name = final_model_name\n",
    "predictor.best_score = best_model_info['Val_Accuracy']\n",
    "\n",
    "predictor.save_best_model('../models/best_model.pkl', preprocessor)\n",
    "\n",
    "print(\"✅ Model saved successfully!\")\n",
    "\n",
    "# Save feature selection artifacts\n",
    "joblib.dump(selector, '../models/feature_selector.pkl')\n",
    "print(\"✅ Feature selector saved!\")\n",
    "\n",
    "# Save updated preprocessor\n",
    "joblib.dump(preprocessor, '../models/preprocessor.pkl')\n",
    "print(\"✅ Updated preprocessor saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model summary report\n",
    "model_summary = {\n",
    "    'model_info': {\n",
    "        'name': final_model_name,\n",
    "        'type': best_model_info['Type'],\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'python_version': sys.version\n",
    "    },\n",
    "    'data_info': {\n",
    "        'training_samples': len(train_data),\n",
    "        'test_samples': len(test_data),\n",
    "        'total_features': len(preprocessor.get_symptom_names()),\n",
    "        'selected_features': len(selected_feature_names),\n",
    "        'classes': len(preprocessor.get_disease_names())\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': float(test_metrics['accuracy']),\n",
    "        'test_precision': float(test_metrics['precision']),\n",
    "        'test_recall': float(test_metrics['recall']),\n",
    "        'test_f1_score': float(test_metrics['f1_score']),\n",
    "        'validation_accuracy': float(val_metrics['accuracy']),\n",
    "        'cross_validation_score': float(best_model_info['CV_Score'])\n",
    "    },\n",
    "    'model_diagnostics': {\n",
    "        'overfitting_score': float(overfitting_score),\n",
    "        'correctly_classified': int(correct_classified.sum()),\n",
    "        'misclassified': int(misclassified.sum()),\n",
    "        'error_rate': float(misclassified.sum() / len(y_test))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add feature importance if available\n",
    "if feature_importance is not None:\n",
    "    model_summary['top_features'] = feature_importance.head(10).to_dict('records')\n",
    "\n",
    "# Save model summary\n",
    "with open('../models/model_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "print(\"✅ Model summary saved to '../models/model_summary.json'\")\n",
    "\n",
    "# Save test predictions for submission\n",
    "test_predictions_df = pd.DataFrame({\n",
    "    'prognosis': y_test_pred\n",
    "})\n",
    "\n",
    "test_predictions_df.to_csv('../predictions.csv', index=False)\n",
    "print(\"✅ Test predictions saved to '../predictions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training summary\n",
    "print(\"🎉 MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"🏆 FINAL MODEL: {final_model_name}\")\n",
    "print(f\"📊 TEST ACCURACY: {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']:.1%})\")\n",
    "print(f\"🎯 TEST F1-SCORE: {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"⚡ CROSS-VALIDATION: {best_model_info['CV_Score']:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 MODEL STATISTICS:\")\n",
    "print(f\"   • Training samples: {len(train_data):,}\")\n",
    "print(f\"   • Features used: {len(selected_feature_names)} / {len(preprocessor.get_symptom_names())}\")\n",
    "print(f\"   • Classes predicted: {len(preprocessor.get_disease_names())}\")\n",
    "print(f\"   • Correct predictions: {correct_classified.sum()} / {len(y_test)}\")\n",
    "\n",
    "print(f\"\\n🔧 TECHNICAL DETAILS:\")\n",
    "print(f\"   • Model type: {type(final_model).__name__}\")\n",
    "print(f\"   • Feature selection: Chi-square (top {len(selected_feature_names)})\")\n",
    "print(f\"   • Cross-validation: 5-fold stratified\")\n",
    "print(f\"   • Overfitting check: {overfitting_score:.4f} ({'✅ Good' if overfitting_score < 0.05 else '⚠️ Monitor'})\")\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "if test_metrics['accuracy'] > 0.95:\n",
    "    print(\"   • Excellent model performance achieved!\")\n",
    "elif test_metrics['accuracy'] > 0.90:\n",
    "    print(\"   • Very good model performance achieved!\")\n",
    "elif test_metrics['accuracy'] > 0.85:\n",
    "    print(\"   • Good model performance achieved!\")\n",
    "else:\n",
    "    print(\"   • Model performance could be improved further.\")\n",
    "\n",
    "if len(selected_feature_names) < len(preprocessor.get_symptom_names()) * 0.8:\n",
    "    print(\"   • Feature selection successfully reduced dimensionality\")\n",
    "\n",
    "if overfitting_score < 0.02:\n",
    "    print(\"   • Model shows excellent generalization\")\n",
    "\n",
    "print(f\"\\n🚀 READY FOR DEPLOYMENT!\")\n",
    "print(\"   All model artifacts have been saved and the model is ready\")\n",
    "print(\"   to be integrated into the Streamlit application.\")\n",
    "\n",
    "print(f\"\\n📁 GENERATED FILES:\")\n",
    "print(\"   • ../models/best_model.pkl (trained model + metadata)\")\n",
    "print(\"   • ../models/preprocessor.pkl (updated with feature selection)\")\n",
    "print(\"   • ../models/feature_selector.pkl (feature selection object)\")\n",
    "print(\"   • ../models/model_summary.json (comprehensive model report)\")\n",
    "print(\"   • ../predictions.csv (test predictions for submission)\")\n",
    "\n",
    "print(f\"\\n⏱️  Training completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
